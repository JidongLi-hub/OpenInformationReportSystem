import subprocess
import os
import sys

# --- é…ç½®åŒºåŸŸ ---
MODEL_PATH = "Qwen/Qwen2-7B-Instruct"  # æ¨¡å‹è·¯å¾„æˆ–åç§°
SERVED_NAME = "Qwen/Qwen2-7B-Instruct" # API æœåŠ¡ä¸­çš„æ¨¡å‹æ˜¾ç¤ºåç§°
PORT = 8002                            # æœåŠ¡ç«¯å£
MAX_MEMORY_USAGE = 1000                # æ˜¾å­˜å ç”¨å°äºæ­¤å€¼(MB)è§†ä¸º"ç©ºé—²"

def get_free_gpus():
    """
    ä½¿ç”¨ nvidia-smi æŸ¥è¯¢æ‰€æœ‰æ˜¾å¡çš„æ˜¾å­˜ä½¿ç”¨æƒ…å†µ
    è¿”å›ç©ºé—²æ˜¾å¡çš„ ID åˆ—è¡¨
    """
    try:
        # æ‰§è¡Œ nvidia-smi æŸ¥è¯¢å‘½ä»¤
        result = subprocess.check_output(
            ["nvidia-smi", "--query-gpu=index,memory.used", "--format=csv,noheader,nounits"],
            encoding="utf-8"
        )
        
        free_gpus = []
        lines = result.strip().split('\n')
        for line in lines:
            index, memory = line.split(',')
            if int(memory.strip()) < MAX_MEMORY_USAGE:
                free_gpus.append(index.strip())
        
        return free_gpus
    except Exception as e:
        print(f"âŒ è·å–æ˜¾å¡çŠ¶æ€å¤±è´¥: {e}")
        sys.exit(1)

def main():
    print("ğŸ” æ­£åœ¨æ‰«ææœåŠ¡å™¨æ˜¾å¡çŠ¶æ€...")
    free_gpus = get_free_gpus()
    
    if not free_gpus:
        print("âŒ é”™è¯¯: å½“å‰æ²¡æœ‰ç©ºé—²çš„æ˜¾å¡ï¼è¯·ç¨åå†è¯•æˆ–æ£€æŸ¥ nvidia-smiã€‚")
        sys.exit(1)
    
    print(f"âœ… å‘ç°ç©ºé—²æ˜¾å¡: {free_gpus}")
    
    # --- å†³ç­–é€»è¾‘ ---
    target_gpus = []
    tp_size = 1
    
    # ç­–ç•¥: ä¼˜å…ˆå‡‘ 2 å¼ å¡åšå¹¶è¡Œï¼Œå¦‚æœä¸å¤Ÿå°±ç”¨ 1 å¼ 
    if len(free_gpus) >= 2:
        target_gpus = free_gpus[:2] # å–å‰ä¸¤å¼ 
        tp_size = 2
        print(f"ğŸš€ ç­–ç•¥: æ˜¾å¡å……è¶³ï¼Œå°†ä½¿ç”¨ GPU {target_gpus} å¼€å¯åŒå¡å¹¶è¡Œæ¨¡å¼ (TP=2)")
    else:
        target_gpus = free_gpus[:1] # å–ç¬¬ä¸€å¼ 
        tp_size = 1
        print(f"âš ï¸ ç­–ç•¥: æ˜¾å¡ç´§å¼ ï¼Œå°†ä½¿ç”¨ GPU {target_gpus} å¼€å¯å•å¡æ¨¡å¼ (TP=1)")

    # --- æ„é€ å¯åŠ¨å‘½ä»¤ ---
    gpu_str = ",".join(target_gpus)
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    env = os.environ.copy()
    env["CUDA_VISIBLE_DEVICES"] = gpu_str
    
    # --- æ ¸å¿ƒä¿®å¤ï¼šæ¸…ç†ä»£ç†é…ç½® ---
    # vLLM å°è¯•é€šè¿‡ç¯å¢ƒå˜é‡é‡Œçš„ä»£ç†è”ç½‘ï¼Œä½†ä»£ç†å¯èƒ½æ²¡å¼€å¯¼è‡´ Connection refusedã€‚
    # è¿™é‡Œå¼ºåˆ¶ç§»é™¤è¿™äº›å˜é‡ï¼Œè®© vLLM ç›´è¿æˆ–ä½¿ç”¨æœ¬åœ°ç¼“å­˜ã€‚
    proxies = ["http_proxy", "https_proxy", "HTTP_PROXY", "HTTPS_PROXY", "all_proxy", "ALL_PROXY"]
    print("-" * 50)
    print("ğŸ§¹ æ­£åœ¨æ¸…ç†ç¯å¢ƒä»£ç†è®¾ç½® (é˜²æ­¢è¿æ¥è¢«æ‹’ç»)...")
    for p in proxies:
        if p in env:
            print(f"   - ç§»é™¤: {p}")
            del env[p]
    
    cmd = [
        "python", "-m", "vllm.entrypoints.openai.api_server",
        "--model", MODEL_PATH,
        "--served-model-name", SERVED_NAME,
        "--trust-remote-code",
        "--tensor-parallel-size", str(tp_size), # åŠ¨æ€è®¾ç½® TP å‚æ•°
        "--port", str(PORT)
    ]
    
    print("-" * 50)
    print(f"æ‰§è¡Œå‘½ä»¤: CUDA_VISIBLE_DEVICES={gpu_str} {' '.join(cmd)}")
    print("-" * 50)
    
    # --- å¯åŠ¨ vLLM ---
    try:
        # ä½¿ç”¨ä¿®æ”¹åçš„ env (æ— ä»£ç†) å¯åŠ¨å­è¿›ç¨‹
        subprocess.run(cmd, env=env, check=True)
    except KeyboardInterrupt:
        print("\nğŸ›‘ æœåŠ¡å·²åœæ­¢ã€‚")
    except subprocess.CalledProcessError as e:
        print(f"\nâŒ æœåŠ¡å¯åŠ¨å‡ºé”™: {e}")

if __name__ == "__main__":
    main()