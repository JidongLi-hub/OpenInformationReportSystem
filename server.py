import os
import re
import socket
import uvicorn
import requests
import time
from fastapi import FastAPI, HTTPException
from fastapi.responses import HTMLResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

# ==========================================
# 1. å°è¯•å¯¼å…¥åŒç›®å½•ä¸‹çš„æ•°æ®åº“æ¨¡å—
# ==========================================
try:
    from database import VectorDatabase
    DB_MODULE_AVAILABLE = True
except ImportError:
    print(f"[Server] âš ï¸ å¯¼å…¥ database æ¨¡å—å¤±è´¥ï¼Œå°†é™çº§è¿è¡Œã€‚")
    DB_MODULE_AVAILABLE = False
    VectorDatabase = None

# ==========================================
# 2. å…¨å±€é…ç½®
# ==========================================

# ã€æ ¸å¿ƒä¿®æ”¹ã€‘ç›´æ¥å†™æ­»è¿™ä¸ªå†·é—¨ç«¯å£ 28888
VLLM_PORT = 28888 
CHAT_API_URL = f"http://localhost:{VLLM_PORT}/v1/chat/completions"
CHAT_MODEL_NAME = "Qwen/Qwen2-7B-Instruct"

# åˆå§‹åŒ–å…¨å±€æ•°æ®åº“å®ä¾‹
GLOBAL_DB = None
if DB_MODULE_AVAILABLE:
    try:
        print("[Server] æ­£åœ¨åˆå§‹åŒ–å‘é‡æ•°æ®åº“æœåŠ¡...")
        GLOBAL_DB = VectorDatabase()
        print("[Server] âœ… å‘é‡æ•°æ®åº“æœåŠ¡å°±ç»ª")
    except Exception as e:
        print(f"[Server] âš ï¸ æ•°æ®åº“å®ä¾‹åˆå§‹åŒ–å¼‚å¸¸: {e}")
        GLOBAL_DB = None

# ==========================================
# 3. FastAPI åº”ç”¨åˆå§‹åŒ–
# ==========================================
app = FastAPI(title="æ€åŠ¿æŠ¥å‘Šç”Ÿæˆç³»ç»Ÿ")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class UserRequest(BaseModel):
    user_prompt: str

# ==========================================
# 4. æ ¸å¿ƒä¸šåŠ¡æ¥å£
# ==========================================

@app.post("/generate_report")
async def generate_report(request: UserRequest):
    print(f"[Server] æ¥æ”¶åˆ†ææŒ‡ä»¤: {request.user_prompt}")
    
    try:
        # --- é˜¶æ®µä¸€ï¼šæƒ…æŠ¥æ£€ç´¢ ---
        retrieved_docs = []
        if GLOBAL_DB:
            try:
                retrieved_docs = GLOBAL_DB.search_embedding(request.user_prompt, top_k=3)
            except Exception as e:
                print(f"[Server] âš ï¸ æ£€ç´¢å¼‚å¸¸: {e}")
        
        if retrieved_docs:
            print(f"[Server] âœ… æ£€ç´¢å®Œæˆï¼Œå¬å› {len(retrieved_docs)} æ¡æ•°æ®ã€‚")
            context_str = "\n".join([f"{i+1}. {doc}" for i, doc in enumerate(retrieved_docs)])
        else:
            print("[Server] âš ï¸ æœªæ£€ç´¢åˆ°æ•°æ®ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ã€‚")
            context_str = "1. [æ¨¡æ‹Ÿ] æ— çº¿ç”µä¿¡å·å¼‚å¸¸å¢å¼ºã€‚\n2. [æ¨¡æ‹Ÿ] æ°”è±¡æµ·å†µæ¶åŠ£ã€‚\n3. [æ¨¡æ‹Ÿ] å†å²åŒæœŸæœ‰æ¼”ç»ƒã€‚"
        
        # --- é˜¶æ®µäºŒï¼šæ„å»º Prompt ---
        final_prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ€åŠ¿åˆ†æå‘˜ã€‚è¯·æ ¹æ®ä»¥ä¸‹ã€èƒŒæ™¯æƒ…æŠ¥ã€‘å¯¹ã€ç”¨æˆ·æŒ‡ä»¤ã€‘è¿›è¡Œæ·±åº¦åˆ†æã€‚
        ã€èƒŒæ™¯æƒ…æŠ¥ã€‘ï¼š{context_str}
        ã€ç”¨æˆ·æŒ‡ä»¤ã€‘ï¼š{request.user_prompt}
        è¦æ±‚ï¼šMarkdownæ ¼å¼ï¼Œåˆ†æ‘˜è¦ã€ç°çŠ¶ã€é¢„æµ‹ä¸‰éƒ¨åˆ†ã€‚
        """

        # --- é˜¶æ®µä¸‰ï¼šå¤§æ¨¡å‹æ¨ç† ---
        llm_content = ""
        try:
            print(f"[Server] è¯·æ±‚ Chat æ¨¡å‹ (ç«¯å£ {VLLM_PORT})...")
            resp = requests.post(
                CHAT_API_URL, 
                json={
                    "model": CHAT_MODEL_NAME,
                    "messages": [{"role": "user", "content": final_prompt}],
                    "temperature": 0.7,
                    "max_tokens": 2048
                }, 
                proxies={"http": None, "https": None},
                timeout=60
            )
            resp.raise_for_status()
            llm_content = resp.json()["choices"][0]["message"]["content"]
            print("[Server] âœ… æŠ¥å‘Šç”Ÿæˆå®Œæˆã€‚")
            
        except Exception as e:
            print(f"[Server] âš ï¸ Chat æ¨¡å‹è°ƒç”¨å¤±è´¥: {e}")
            llm_content = f"> **âš ï¸ ç³»ç»Ÿæç¤º**ï¼šå¤§æ¨¡å‹è¿æ¥å¼‚å¸¸ (Port {VLLM_PORT})ã€‚\n\n**ç›¸å…³æƒ…æŠ¥ï¼š**\n{context_str}"

        return {
            "status": "success",
            "original_query": request.user_prompt,
            "retrieved_info": context_str,
            "report_content": llm_content
        }

    except Exception as e:
        print(f"[Server] âŒ å†…éƒ¨é”™è¯¯: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ==========================================
# 5. é™æ€èµ„æºæ‰˜ç®¡
# ==========================================

@app.get("/", response_class=HTMLResponse)
async def serve_frontend():
    frontend_path = "index.html"
    if not os.path.exists(frontend_path):
        return "<h1>é”™è¯¯ï¼šæœªæ‰¾åˆ° index.html</h1>"
    
    with open(frontend_path, "r", encoding="utf-8") as f:
        content = f.read()
    
    content = re.sub(
        r'fetch\s*\(\s*["\']http://localhost:\d+/generate_report["\']', 
        'fetch("/generate_report"', 
        content
    )
    return HTMLResponse(content=content)

# ä¸ºäº†æ›´å®‰å…¨ï¼Œæˆ‘ä»¬ä»ä¸€ä¸ªå†·é—¨çš„é«˜ä½ç«¯å£å¼€å§‹æ‰¾
DEFAULT_SERVER_PORT = 28001

def find_free_port(start_port=DEFAULT_SERVER_PORT, max_retries=100):
    port = start_port
    while port < start_port + max_retries:
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
            if s.connect_ex(('localhost', port)) != 0:
                try:
                    s.bind(('0.0.0.0', port))
                    return port
                except OSError: pass
        port += 1
    raise RuntimeError("æ— æ³•åˆ†é…ç«¯å£")

if __name__ == "__main__":
    try:
        PORT = find_free_port(DEFAULT_SERVER_PORT)
        print("="*60)
        print(f"ğŸš€ æ€åŠ¿æŠ¥å‘Šç”Ÿæˆç³»ç»Ÿå¯åŠ¨")
        print(f"ğŸ”— è®¿é—®åœ°å€: http://localhost:{PORT}")
        print(f"ğŸ”— æ¨¡å‹ç«¯å£: {VLLM_PORT} (å›ºå®š)")
        print("="*60)
        uvicorn.run(app, host="0.0.0.0", port=PORT)
    except Exception as e:
        print(f"âŒ å¯åŠ¨å¤±è´¥: {e}")