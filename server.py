import os
import re
import socket
import uvicorn
import requests
import time
from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import HTMLResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

# ==========================================
# 1. æ ¸å¿ƒé…ç½®ä¸å·¥å…·å‡½æ•°
# ==========================================

# æŒ‡å‘ vLLM æœåŠ¡çš„åœ°å€
# âš ï¸ è¯·ç¡®ä¿è¿™é‡Œæ˜¯ä½ çœŸå® vLLM è¿è¡Œçš„ç«¯å£ (ä½ ä¹‹å‰è¯´æ˜¯ 8002)
VLLM_API_URL = "http://localhost:8002/v1/chat/completions"
MODEL_NAME = "Qwen/Qwen2-7B-Instruct"

def find_free_port(start_port=8001, max_retries=100):
    """è‡ªåŠ¨å¯»æ‰¾ç©ºé—²ç«¯å£"""
    port = start_port
    while port < start_port + max_retries:
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
            if s.connect_ex(('localhost', port)) != 0:
                try:
                    s.bind(('0.0.0.0', port))
                    return port
                except OSError:
                    pass
        port += 1
    raise RuntimeError("æ‰¾ä¸åˆ°å¯ç”¨çš„ç©ºé—²ç«¯å£ï¼")

# ==========================================
# 2. FastAPI åº”ç”¨åˆå§‹åŒ–
# ==========================================
app = FastAPI(title="æ€åŠ¿æŠ¥å‘Šç”Ÿæˆç³»ç»Ÿ")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ==========================================
# 3. ä¸šåŠ¡é€»è¾‘ (æ•°æ®åº“æ¨¡æ‹Ÿ & æŠ¥å‘Šç”Ÿæˆ)
# ==========================================

class UserRequest(BaseModel):
    user_prompt: str

def mock_search_database(query: str):
    """æ¨¡æ‹Ÿæ•°æ®åº“æ£€ç´¢"""
    print(f"[åç«¯æ—¥å¿—] æ­£åœ¨æ•°æ®åº“ä¸­æ£€ç´¢: {query}") 
    return [
        "1. [æƒ…æŠ¥æºA] ç›‘æµ‹æ•°æ®æ˜¾ç¤ºï¼Œè¿‡å»48å°æ—¶å†…ï¼Œç›®æ ‡æµ·åŸŸçš„æ— çº¿ç”µé€šä¿¡é¢‘ç‡æ¯”å¹³æ—¶å¢åŠ äº† 15%ã€‚",
        "2. [æƒ…æŠ¥æºB] æ°”è±¡éƒ¨é—¨é¢„æŠ¥ï¼Œå—å­£é£ä½å‹å½±å“ï¼Œè¯¥åŒºåŸŸæœªæ¥ä¸‰å¤©å°†å‡ºç° 4-5 ç±³çš„å·¨æµªã€‚",
        "3. [æƒ…æŠ¥æºC] å†å²ç±»ä¼¼äº‹ä»¶å›é¡¾ï¼šå»å¹´åŒæœŸï¼Œå‘¨è¾¹å›½å®¶æ›¾åœ¨æ­¤æµ·åŸŸè¿›è¡Œè¿‡è”åˆæ¼”ç»ƒã€‚"
    ]

@app.post("/generate_report")
async def generate_report(request: UserRequest):
    """ç”ŸæˆæŠ¥å‘Šçš„æ ¸å¿ƒ API"""
    print(f"[åç«¯æ—¥å¿—] æ”¶åˆ°æŒ‡ä»¤: {request.user_prompt}")
    
    try:
        # RAG æ£€ç´¢
        retrieved_docs = mock_search_database(request.user_prompt)
        context_str = "\n".join(retrieved_docs)
        
        # ç»„è£… Prompt
        final_prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ€åŠ¿åˆ†æå‘˜ã€‚è¯·æ ¹æ®ä»¥ä¸‹ã€èƒŒæ™¯ä¿¡æ¯ã€‘å’Œã€ç”¨æˆ·æŒ‡ä»¤ã€‘ï¼Œæ’°å†™ä¸€ä»½ä¸“ä¸šçš„æ€åŠ¿æŠ¥å‘Šã€‚
        ã€èƒŒæ™¯ä¿¡æ¯ã€‘ï¼š{context_str}
        ã€ç”¨æˆ·æŒ‡ä»¤ã€‘ï¼š{request.user_prompt}
        è¦æ±‚ï¼šMarkdownæ ¼å¼ï¼Œå­—æ•°500ä»¥å†…ï¼Œåˆ†æ¡åˆ—è¿°ã€‚
        """

        # è°ƒç”¨ vLLM (å¸¦é™çº§å¤„ç†)
        llm_content = ""
        try:
            print(f"[åç«¯æ—¥å¿—] æ­£åœ¨è¯·æ±‚ vLLM (ç«¯å£ 8002)... è¯·è€å¿ƒç­‰å¾…...")
            resp = requests.post(
                VLLM_API_URL, 
                json={
                    "model": MODEL_NAME,
                    "messages": [{"role": "user", "content": final_prompt}],
                    "temperature": 0.7,
                    "max_tokens": 2048
                }, 
                proxies={"http": None, "https": None},
                # ====================================================
                # ğŸ‘‡ã€æ ¸å¿ƒä¿®æ”¹ã€‘å°†è¶…æ—¶æ—¶é—´ä» 5 ç§’æ”¹ä¸º 60 ç§’
                # å¤§æ¨¡å‹ç”Ÿæˆéœ€è¦æ—¶é—´ï¼Œ5ç§’å¤ªçŸ­äº†
                # ====================================================
                timeout=60 
            )
            resp.raise_for_status()
            llm_content = resp.json()["choices"][0]["message"]["content"]
            print("[åç«¯æ—¥å¿—] âœ… vLLM ç”ŸæˆæˆåŠŸï¼")
            
        except requests.exceptions.Timeout:
            print(f"[åç«¯æ—¥å¿—] âš ï¸ vLLM å“åº”è¶…æ—¶ (>60s) -> åˆ‡æ¢åˆ°æ¨¡æ‹Ÿæ¨¡å¼")
            llm_content = self._get_fallback_content()
            
        except Exception as e:
            print(f"[åç«¯æ—¥å¿—] âš ï¸ vLLM è°ƒç”¨å‡ºé”™: {e} -> åˆ‡æ¢åˆ°æ¨¡æ‹Ÿæ¨¡å¼")
            llm_content = self._get_fallback_content()

        # å¦‚æœ llm_content ä¸ºç©ºï¼ˆæ¯”å¦‚ try å—æœªå®Œå…¨æ‰§è¡Œï¼‰ï¼Œèµ‹äºˆé»˜è®¤å€¼
        if not llm_content:
             llm_content = self._get_fallback_content()

        return {
            "status": "success",
            "original_query": request.user_prompt,
            "retrieved_info": context_str,
            "report_content": llm_content
        }

    except Exception as e:
        print(f"[é”™è¯¯] {e}")
        raise HTTPException(status_code=500, detail=str(e))

    def _get_fallback_content(self):
        """è¿”å›é™çº§ç”¨çš„æ¨¡æ‹Ÿæ•°æ®"""
        return """
> **âš ï¸ ç³»ç»Ÿæç¤º**ï¼šæ¨¡å‹æœåŠ¡å“åº”è¶…æ—¶æˆ–ä¸å¯ç”¨ï¼Œä»¥ä¸‹ä¸ºè§„åˆ™å¼•æ“ç”Ÿæˆçš„æ¨¡æ‹Ÿæ•°æ®ã€‚

## ğŸ“Š æ€åŠ¿åˆ†ææŠ¥å‘Šï¼ˆç¦»çº¿ç‰ˆï¼‰
æ ¹æ®æ£€ç´¢åˆ°çš„æƒ…æŠ¥ï¼ˆæ— çº¿ç”µé¢‘ç‡å¢åŠ ã€æ¶åŠ£æµ·å†µï¼‰ï¼Œåˆ¤æ–­å½“å‰åŒºåŸŸå­˜åœ¨**éå…¸å‹å†›äº‹æ´»åŠ¨**ç‰¹å¾ã€‚
å»ºè®®æŒç»­å…³æ³¨æ°”è±¡çª—å£æœŸï¼ˆæœªæ¥72å°æ—¶ï¼‰ã€‚
"""

# ä¸ºäº†å…¼å®¹å‡½æ•°å†…è°ƒç”¨ï¼Œå®šä¹‰ä¸€ä¸ªç‹¬ç«‹çš„ fallback å‡½æ•°
def _get_fallback_content_standalone():
    return """
> **âš ï¸ ç³»ç»Ÿæç¤º**ï¼šæ¨¡å‹æœåŠ¡å“åº”è¶…æ—¶æˆ–ä¸å¯ç”¨ï¼Œä»¥ä¸‹ä¸ºè§„åˆ™å¼•æ“ç”Ÿæˆçš„æ¨¡æ‹Ÿæ•°æ®ã€‚

## ğŸ“Š æ€åŠ¿åˆ†ææŠ¥å‘Šï¼ˆç¦»çº¿ç‰ˆï¼‰
æ ¹æ®æ£€ç´¢åˆ°çš„æƒ…æŠ¥ï¼ˆæ— çº¿ç”µé¢‘ç‡å¢åŠ ã€æ¶åŠ£æµ·å†µï¼‰ï¼Œåˆ¤æ–­å½“å‰åŒºåŸŸå­˜åœ¨**éå…¸å‹å†›äº‹æ´»åŠ¨**ç‰¹å¾ã€‚
å»ºè®®æŒç»­å…³æ³¨æ°”è±¡çª—å£æœŸï¼ˆæœªæ¥72å°æ—¶ï¼‰ã€‚
"""

# ä¿®æ­£ generate_report å†…éƒ¨çš„è°ƒç”¨
@app.post("/generate_report")
async def generate_report_fixed(request: UserRequest):
    print(f"[åç«¯æ—¥å¿—] æ”¶åˆ°æŒ‡ä»¤: {request.user_prompt}")
    
    try:
        retrieved_docs = mock_search_database(request.user_prompt)
        context_str = "\n".join(retrieved_docs)
        
        final_prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ€åŠ¿åˆ†æå‘˜ã€‚è¯·æ ¹æ®ä»¥ä¸‹ã€èƒŒæ™¯ä¿¡æ¯ã€‘å’Œã€ç”¨æˆ·æŒ‡ä»¤ã€‘ï¼Œæ’°å†™ä¸€ä»½ä¸“ä¸šçš„æ€åŠ¿æŠ¥å‘Šã€‚
        ã€èƒŒæ™¯ä¿¡æ¯ã€‘ï¼š{context_str}
        ã€ç”¨æˆ·æŒ‡ä»¤ã€‘ï¼š{request.user_prompt}
        è¦æ±‚ï¼šMarkdownæ ¼å¼ï¼Œå­—æ•°500ä»¥å†…ï¼Œåˆ†æ¡åˆ—è¿°ã€‚
        """

        llm_content = ""
        try:
            print(f"[åç«¯æ—¥å¿—] æ­£åœ¨è¯·æ±‚ vLLM (ç«¯å£ 8002)... è¯·è€å¿ƒç­‰å¾…...")
            resp = requests.post(
                VLLM_API_URL, 
                json={
                    "model": MODEL_NAME,
                    "messages": [{"role": "user", "content": final_prompt}],
                    "temperature": 0.7,
                    "max_tokens": 2048
                }, 
                proxies={"http": None, "https": None},
                # ã€ä¿®æ”¹ã€‘è¶…æ—¶æ—¶é—´æ”¹ä¸º 60 ç§’
                timeout=60 
            )
            resp.raise_for_status()
            llm_content = resp.json()["choices"][0]["message"]["content"]
            print("[åç«¯æ—¥å¿—] âœ… vLLM ç”ŸæˆæˆåŠŸï¼")
            
        except Exception as e:
            print(f"[åç«¯æ—¥å¿—] âš ï¸ vLLM è°ƒç”¨å¼‚å¸¸: {e} -> åˆ‡æ¢åˆ°æ¨¡æ‹Ÿæ¨¡å¼")
            llm_content = _get_fallback_content_standalone()

        return {
            "status": "success",
            "original_query": request.user_prompt,
            "retrieved_info": context_str,
            "report_content": llm_content
        }

    except Exception as e:
        print(f"[é”™è¯¯] {e}")
        raise HTTPException(status_code=500, detail=str(e))

# è¦†ç›–ä¹‹å‰çš„è·¯ç”±å®šä¹‰
app.router.routes = [r for r in app.router.routes if r.path != "/generate_report"]
app.post("/generate_report")(generate_report_fixed)


# ==========================================
# 4. å‰ç«¯æ‰˜ç®¡ (æ ¸å¿ƒé»‘ç§‘æŠ€)
# ==========================================

@app.get("/", response_class=HTMLResponse)
async def serve_frontend():
    frontend_path = "index.html"
    if not os.path.exists(frontend_path):
        return "<h1>é”™è¯¯ï¼šæ‰¾ä¸åˆ° index.html æ–‡ä»¶</h1><p>è¯·ç¡®ä¿ index.html å’Œ server.py åœ¨åŒä¸€ç›®å½•ä¸‹ã€‚</p>"
    
    with open(frontend_path, "r", encoding="utf-8") as f:
        content = f.read()
    
    content = re.sub(
        r'fetch\s*\(\s*["\']http://localhost:\d+/generate_report["\']', 
        'fetch("/generate_report"', 
        content
    )
    
    return HTMLResponse(content=content)

# ==========================================
# 5. å¯åŠ¨å…¥å£
# ==========================================

if __name__ == "__main__":
    try:
        PORT = find_free_port(8001)
    except Exception as e:
        print(f"âŒ {e}")
        exit(1)

    print("="*50)
    print(f"ğŸš€ ç³»ç»Ÿå¯åŠ¨æˆåŠŸï¼")
    print(f"ğŸŒ è®¿é—®åœ°å€: http://localhost:{PORT}")
    print(f"ğŸ”Œ åç«¯ç«¯å£: {PORT} (å‰ç«¯å·²è‡ªåŠ¨é›†æˆ)")
    print("="*50)
    
    uvicorn.run(app, host="0.0.0.0", port=PORT)