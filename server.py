from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware  # <---ã€æ–°å¢žã€‘å¯¼å…¥CORSåº“
from pydantic import BaseModel
import requests
import uvicorn

# åˆå§‹åŒ– FastAPI åº”ç”¨
app = FastAPI(title="æ€åŠ¿æŠ¥å‘Šç”Ÿæˆç³»ç»ŸåŽç«¯")

# ==========================================
# ðŸ‘‡ã€æ–°å¢žã€‘é…ç½®è·¨åŸŸå…è®¸ (CORS)
# ==========================================
app.add_middleware(
    CORSMiddleware,
    # å…è®¸æ‰€æœ‰æ¥æºè®¿é—®ï¼ˆæ¯”å¦‚ http://localhost:8501ï¼‰
    # åœ¨ç”Ÿäº§çŽ¯å¢ƒä¸­é€šå¸¸ä¼šæŒ‡å®šå…·ä½“çš„åŸŸåï¼Œä½†åœ¨å¼€å‘æµ‹è¯•é˜¶æ®µç”¨ "*" æœ€æ–¹ä¾¿
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],  # å…è®¸æ‰€æœ‰ HTTP æ–¹æ³• (GET, POSTç­‰)
    allow_headers=["*"],  # å…è®¸æ‰€æœ‰ HTTP å¤´
)
# ==========================================

# --- é…ç½®éƒ¨åˆ† ---
# æŒ‡å‘ vLLM æœåŠ¡çš„åœ°å€ï¼ˆä¿ç•™ä½ ä»£ç ä¸­çš„ 8002 ç«¯å£ï¼‰
VLLM_API_URL = "http://localhost:8002/v1/chat/completions"
MODEL_NAME = "Qwen/Qwen2-7B-Instruct"

# --- 1. å®šä¹‰æ•°æ®æ¨¡åž‹ ---
class UserRequest(BaseModel):
    user_prompt: str

# --- 2. æ¨¡æ‹Ÿæ•°æ®åº“æŸ¥è¯¢ ---
def mock_search_database(query: str):
    print(f"[åŽç«¯æ—¥å¿—] æ­£åœ¨æ•°æ®åº“ä¸­æ£€ç´¢å…³é”®è¯: {query} ...") 
    return "ã€æ£€ç´¢ç»“æžœã€‘ï¼šè¿‘æœŸè¯¥åŒºåŸŸæœ‰é¢‘ç¹çš„æµ·ä¸Šæ´»åŠ¨ï¼Œä¸”ä¼´éšæœ‰å­£é£Žæ°”å€™å½±å“ã€‚å¤šæ–¹åŠ¿åŠ›åœ¨æ­¤è¿›è¡Œäº†å¸¸è§„å·¡èˆªã€‚"

# --- 3. æ ¸å¿ƒæŽ¥å£ï¼šç”ŸæˆæŠ¥å‘Š ---
@app.post("/generate_report")
async def generate_report(request: UserRequest):
    print(f"[åŽç«¯æ—¥å¿—] æ”¶åˆ°å‰ç«¯è¯·æ±‚: {request.user_prompt}")
    
    try:
        # æ­¥éª¤ A: åŽ»æ•°æ®åº“æŸ¥èµ„æ–™
        retrieved_info = mock_search_database(request.user_prompt)
        
        # æ­¥éª¤ B: ç»„è£… Prompt
        final_prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ€åŠ¿åˆ†æžå‘˜ã€‚è¯·æ ¹æ®ä»¥ä¸‹ã€èƒŒæ™¯ä¿¡æ¯ã€‘å’Œã€ç”¨æˆ·æŒ‡ä»¤ã€‘ï¼Œæ’°å†™ä¸€ä»½ä¸“ä¸šçš„æ€åŠ¿æŠ¥å‘Šã€‚
        
        ã€èƒŒæ™¯ä¿¡æ¯ã€‘ï¼š
        {retrieved_info}
        
        ã€ç”¨æˆ·æŒ‡ä»¤ã€‘ï¼š
        {request.user_prompt}
        
        è¦æ±‚ï¼š
        1. æŠ¥å‘Šæ ¼å¼åŒ…å«ï¼šæ‘˜è¦ã€çŽ°çŠ¶åˆ†æžã€è¶‹åŠ¿é¢„æµ‹ã€‚
        2. è¯­æ°”ä¸“ä¸šã€å®¢è§‚ã€‚
        3. å­—æ•°æŽ§åˆ¶åœ¨ 500 å­—ä»¥å†…ï¼ˆæµ‹è¯•ç”¨ï¼‰ã€‚
        4. ä½¿ç”¨ Markdown æ ¼å¼ã€‚
        """

        # æ­¥éª¤ C: å‡†å¤‡ payload
        payload = {
            "model": MODEL_NAME,
            "messages": [
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": final_prompt}
            ],
            "temperature": 0.7,
            "max_tokens": 2048
        }

        # æ­¥éª¤ D: è¯·æ±‚ vLLM
        print("[åŽç«¯æ—¥å¿—] æ­£åœ¨è¯·æ±‚ vLLM æ¨¡åž‹...")
        response = requests.post(
            VLLM_API_URL, 
            json=payload, 
            proxies={"http": None, "https": None}
        )
        response.raise_for_status() 
        
        llm_content = response.json()["choices"][0]["message"]["content"]
        print("[åŽç«¯æ—¥å¿—] æ¨¡åž‹ç”Ÿæˆå®Œæ¯•ã€‚")

        # æ­¥éª¤ E: è¿”å›žç»“æžœ
        return {
            "status": "success",
            "original_query": request.user_prompt,
            "retrieved_info": retrieved_info,
            "report_content": llm_content
        }

    except Exception as e:
        print(f"[é”™è¯¯] å¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    # å¯åŠ¨æœåŠ¡å™¨åœ¨ 8001 ç«¯å£
    uvicorn.run(app, host="0.0.0.0", port=8001)