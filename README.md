# 开源情报项目


## 数据处理
1. 格式转换
爬取的原始文档，大多为epub(电子书)或者pdf格式，难以直接用于分块处理和向量化。
我们首先使用Pandoc工具将epub转换为pdf格式。


接着使用marker将所有的pdf识别为带有层级格式的markdown文件。
```bash

```

2. 分块处理
为了便于后续的向量化处理，我们将markdown文件进行分块处理。

## 智能分析与态势感知系统部署

本模块负责将处理好的数据与大模型结合（RAG），生成智能态势报告，并通过可视化界面呈现。系统采用前后端分离但一体化部署的架构。

### 1. 环境准备

确保服务器已安装以下 Python 依赖库：

```bash
# 基础服务依赖
pip install fastapi uvicorn requests python-multipart

# 向量数据库与AI接口依赖
pip install pymilvus openai

# 大模型推理依赖 (如果尚未安装)
pip install vllm
```

### 2. 启动大模型推理服务 (vLLM)

系统使用 `Qwen/Qwen2-7B-Instruct` 作为基座模型。为了避免端口冲突和显存抢占，请使用我们封装好的自动化脚本启动。

* **脚本名称**: `start_vllm.py`
* **功能**: 自动检测空闲显卡，自动避让端口，强制离线模式（解决 HF 连通性问题）。
* **默认端口**: `28888` (固定端口，避免与常用端口冲突)

**启动命令：**

```bash
python start_vllm.py
```

注意: 启动成功后，脚本会保持运行，请不要关闭终端窗口。

### 3. 启动应用服务 (后端 API + 前端界面)

`server.py` 集成了 FastAPI 后端业务逻辑和静态前端托管。它会自动连接数据库和上述的大模型服务。

* **脚本名称**: `server.py`
* **依赖文件**: 确保 `index.html` 和 `database.py` 与 `server.py` 在同一目录下。
* **默认端口**: `28001` (如果被占用会自动递增寻找，如 28002)

**启动命令：**

```bash
python server.py
```

启动成功后，终端会显示访问地址，例如：

```text
==================================================
🚀 态势报告生成系统启动
🔗 访问地址: http://localhost:28001
🔗 模型端口: 28888 (固定)
==================================================
```

### 4. 浏览器访问

由于服务器位于远程环境，推荐使用 SSH 隧道将端口映射到本地浏览器访问。

**SSH 端口映射命令 (在本地电脑执行):**

```bash
# 将服务器的 28001 端口映射到本地
# 请替换 '用户名@服务器IP' 为实际信息
ssh -L 28001:localhost:28001 用户名@服务器IP
```

映射成功后，在本地浏览器打开：
👉 **http://localhost:28001**

### 5. 功能说明

* **指令控制台**: 输入自然语言指令（如“分析美墨边境局势”）。
* **态势分析报告**: 左侧 Tab 展示大模型基于 RAG 生成的结构化报告（Markdown 渲染）。
* **原始情报溯源**: 右侧 Tab 展示从向量数据库检索到的原始数据片段，支持溯源核查。